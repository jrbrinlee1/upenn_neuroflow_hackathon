{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Categorize Youtube Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial videod data only categroized roughly 20% of videos. This notebook uses those categorized videos to categorize the additional 80%. \n",
    "\n",
    "First, transcripts are pulling in from YouTube and it is added to data already avaliable such as tags, short descriptions, etc. Then text is cleaned - first it is tokenized, then stopwords were removed using nltk, and then words were stemmed. This was done after the initial models predictions scores were too low.\n",
    "\n",
    "After the data was cleaned we used GridSearchCV along with CountVectorizer, tfidfTranformer, and different ML models to determine which model worked the best with which paramters. For here, the final model was produced and video feature space was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def get_youtube_video_id(url):\n",
    "    # Examples:\n",
    "    # - http://youtu.be/SA2iWivDJiE\n",
    "    # - http://www.youtube.com/watch?v=_oPAwA_Udwc&feature=feedu\n",
    "    # - http://www.youtube.com/embed/SA2iWivDJiE\n",
    "    # - http://www.youtube.com/v/SA2iWivDJiE?version=3&amp;hl=en_US\n",
    "    query = urlparse(url)\n",
    "    if query.hostname == 'youtu.be': return query.path[1:]\n",
    "    if query.hostname in {'www.youtube.com', 'youtube.com'}:\n",
    "        if query.path == '/watch': return parse_qs(query.query)['v'][0]\n",
    "        if query.path[:7] == '/embed/': return query.path.split('/')[2]\n",
    "        if query.path[:3] == '/v/': return query.path.split('/')[2]\n",
    "    # fail?\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def get_youtube_video_transcript(video_id):\n",
    "    \n",
    "    transcript_text = \"\"\n",
    "\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "    for dic in transcript_list:\n",
    "        print(dic.get('text'))\n",
    "        transcript_text = transcript_text + dic.get('text')\n",
    "    \n",
    "    return transcript_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_list = [w for w in word_list if not w in stop_words]\n",
    "    \n",
    "    new_list = []\n",
    "    \n",
    "    for w in word_list:\n",
    "        if w not in stop_words:\n",
    "            new_list.append(w)\n",
    "    \n",
    "    #remove additional words\n",
    "    word_list = [i for i in word_list if i != \"www\"]\n",
    "    word_list = [i for i in word_list if i != \"http\"]\n",
    "    word_list = [i for i in word_list if i != \"com\"]\n",
    "    word_list = [i for i in word_list if i != \"https\"]\n",
    "    word_list = [i for i in word_list if i != \"goo\"]\n",
    "    word_list = [i for i in word_list if i != \"gl\"]\n",
    "    word_list = [i for i in word_list if i != \"The\"]\n",
    "    word_list = [i for i in word_list if i != \"nan\"]\n",
    "    word_list = [i for i in word_list if i != \"youtube\"]\n",
    "    word_list = [i for i in word_list if i != \"ly\"]\n",
    "    word_list = [i for i in word_list if i != \"â\"]\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(list):\n",
    "    \n",
    "    str = \" \"\n",
    "    \n",
    "    for element in list:\n",
    "        str = str + \" \" + element\n",
    "            \n",
    "    return str\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# pip install spacy nltk\n",
    "# python -m spacy download en\n",
    "\n",
    "def stem_words(word_list):\n",
    "    \n",
    "    new_list = []\n",
    "    \n",
    "    if(word_list is None):\n",
    "        return new_list\n",
    "    \n",
    "    en_nlp = spacy.load('en_core_web_sm')\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    #doc_spacy = en_nlp(word_list)   \n",
    "    \n",
    "    for word in word_list:\n",
    "        #print(\"word before stemming is \": )\n",
    "        \n",
    "        stemmed = stemmer.stem(word)\n",
    "        new_list.append(stemmed)\n",
    "    \n",
    "    \n",
    "    return new_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = pd.read_csv(\"patient_info.csv\", error_bad_lines = False, engine = \"python\")\n",
    "video_df = pd.read_csv(r\"video_watched.csv\", error_bad_lines=False, engine = \"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Video Data & Feature Engineering for Video Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code cleans video data to ues the bag of words method to categroize each video. This data is primarily missing in the original data but should help in recommending movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code directly below was used to pull transcripts from youtube videos but was then stored in excel file for easier access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_data = video_df[[ 'video_id', 'url', 'primary_category', 'secondary_category', 'notes', 'description', 'tags', 'length']]\n",
    "#video_data = video_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_data['youtube_id'] = video_data.apply(lambda x : get_youtube_video_id(x['url']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#video_data['transcript'] = video_data.apply(lambda x : get_youtube_video_transcript(x['youtube_id']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_data.to_excel(\"video_data_with_transcripts.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin cleaning data to train video classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_cls_data = pd.read_excel(r\"video_data_with_transcripts.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is to do a sanity check on the methods used to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = video_cls_data[:5]\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['tok'] = test.apply(lambda x : tokenizer.tokenize(x['all text']), axis = 1)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['nonstop'] = test.apply(lambda x : remove_stopwords(x['tok']), axis = 1)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['stem'] = test.apply(lambda x : stem_words(x['nonstop']), axis = 1)\n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is to tokenize sentences, remove stopwords, and stemming - this is was store in excel file and commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW SO DON'T RUN UNLESS ERROR ###\n",
    "# read from excel instead\n",
    "\n",
    "# tokenize text\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "video_cls_data['words'] = video_cls_data.apply(lambda x : tokenizer.tokenize(x['all text']), axis = 1)\n",
    "# remove stop words\n",
    "video_cls_data['words'] = video_cls_data.apply(lambda x : remove_stopwords(x['words']), axis = 1)\n",
    "# stem words\n",
    "video_cls_data['words'] = video_cls_data.apply(lambda x : stem_words(x['words']), axis = 1)\n",
    "# remove original data\n",
    "del video_cls_data['all text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['get', 'first', '30', 'day', 'curios', 'strea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>['sleep', 'cloudlessmind', 'whi', 'worri', 'mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['To', 'support', 'channel', 'level', 'health'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>771</td>\n",
       "      <td>Cognitive Behavioral Therapy</td>\n",
       "      <td>['depress', 'anxieti', 'stress', 'manag', 'pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['welcom', 'neuroflow', 'tool', 'allow', 'work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1018</td>\n",
       "      <td>6464</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['subscrib', 'ðŸ', 'offici', 'bbc', 'youtub', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1019</td>\n",
       "      <td>6463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['understand', 'cope', 'skill', 'hello', 'welc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1020</td>\n",
       "      <td>6449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['neuroflow', 'amc', 'partnership', 'video', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1021</td>\n",
       "      <td>6452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['subscrib', 'ðŸ', 'offici', 'bbc', 'youtub', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1022</td>\n",
       "      <td>6450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['thi', 'video', 'well', 'defend', 'imovi', 'w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1023 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  video_id              primary_category  \\\n",
       "0              0       624                           NaN   \n",
       "1              1       620                       Anxiety   \n",
       "2              2       787                           NaN   \n",
       "3              3       771  Cognitive Behavioral Therapy   \n",
       "4              4      2587                           NaN   \n",
       "...          ...       ...                           ...   \n",
       "1018        1018      6464                           NaN   \n",
       "1019        1019      6463                           NaN   \n",
       "1020        1020      6449                           NaN   \n",
       "1021        1021      6452                           NaN   \n",
       "1022        1022      6450                           NaN   \n",
       "\n",
       "                                                  words  \n",
       "0     ['get', 'first', '30', 'day', 'curios', 'strea...  \n",
       "1     ['sleep', 'cloudlessmind', 'whi', 'worri', 'mu...  \n",
       "2     ['To', 'support', 'channel', 'level', 'health'...  \n",
       "3     ['depress', 'anxieti', 'stress', 'manag', 'pai...  \n",
       "4     ['welcom', 'neuroflow', 'tool', 'allow', 'work...  \n",
       "...                                                 ...  \n",
       "1018  ['subscrib', 'ðŸ', 'offici', 'bbc', 'youtub', ...  \n",
       "1019  ['understand', 'cope', 'skill', 'hello', 'welc...  \n",
       "1020  ['neuroflow', 'amc', 'partnership', 'video', '...  \n",
       "1021  ['subscrib', 'ðŸ', 'offici', 'bbc', 'youtub', ...  \n",
       "1022  ['thi', 'video', 'well', 'defend', 'imovi', 'w...  \n",
       "\n",
       "[1023 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_cls_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't update this file unless data is changed for the good\n",
    "#video_cls_data.to_excel(\"processed_video_data_for_bag_of_words.xlsx\")\n",
    "#this was done else where and an 'as string' column was added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin to process to train prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_cls_data = pd.read_excel(\"processed_video_data_for_bag_of_words.xlsx\")\n",
    "\n",
    "# remove rows with no category because they cant help train supervised model\n",
    "train_rows = video_cls_data.dropna()\n",
    "# split data from category\n",
    "X_words = train_rows['as string']\n",
    "y = train_rows['primary_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_train, X_words_test, y_train, y_test = train_test_split(X_words, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = CountVectorizer(list(X_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = test.fit_transform(list(X_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = test.transform(list(X_words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       sleep whi We worri So much cloudlessmind whi ...\n",
       "3       depress anxieti stress manag pain substanc us...\n",
       "5       stress 4 7 8 breath exercis gozen gozen thi g...\n",
       "12      stop neg self talk now guid exercis thi guid ...\n",
       "14      how Be more confid you alway get want tri lea...\n",
       "                             ...                        \n",
       "572     amaz effect gratitud thank you watch braincra...\n",
       "640     7 pregnanc warn sign are awar sign pregnanc w...\n",
       "642     substanc use disord visit us khanacademi org ...\n",
       "861     spotlight peer specialist who are they A cert...\n",
       "862     substanc use disord present treatment underst...\n",
       "Name: as string, Length: 203, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used GridSearchCV to test different models and parameters in each model. Overall, Linear SVC was the best with a C value of roughly 0.03, not using term frequency-inverse document frequency, and n-gram of 1 and only 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681707317073171\n",
      "{'lin_svc__C': 0.03359818286283781, 'tfidf__use_idf': False, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(steps = [('vectorizer', CountVectorizer()), ('tfidf', TfidfTransformer()), ('lin_svc', LinearSVC(class_weight=\"balanced\"))])\n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1,1), (1,2), (2,2)], 'tfidf__use_idf': (True, False), 'lin_svc__C': np.logspace(-4,4,20)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs = -1)\n",
    "gs_clf_svm = gs_clf_svm.fit(list(X_words), np.array(y))\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with RBF kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7240243902439024\n",
      "{'svc__C': 545.5594781168514, 'svc__gamma': 0.0018329807108324356, 'tfidf__use_idf': False, 'vectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(steps = [('vectorizer', CountVectorizer()), ('tfidf', TfidfTransformer()), ('svc', SVC(kernel = 'rbf', class_weight=\"balanced\"))])\n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1,1), (1,2), (2,2)], 'tfidf__use_idf': (True, False), 'svc__C': np.logspace(-4,4,20), 'svc__gamma': np.logspace(-4,4,20)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs = -1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_words, np.array(y))\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482926829268293\n",
      "{'log_reg__C': 21.54434690031882, 'log_reg__solver': 'newton-cg', 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(steps = [('vectorizer', CountVectorizer()), ('tfidf', TfidfTransformer()), ('log_reg', LogisticRegression(class_weight=\"balanced\"))])\n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1,1), (1,2), (2,2)], 'tfidf__use_idf': (True, False), 'log_reg__C': np.logspace(-4,4,4), 'log_reg__solver': ('newton-cg', 'sag')}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs = -1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_words, np.array(y))\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6456097560975609\n",
      "{'multiNB__alpha': 0.0006614740641230146, 'tfidf__use_idf': False, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(steps = [('vectorizer', CountVectorizer()), ('tfidf', TfidfTransformer()), ('multiNB',  MultinomialNB())])\n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1,1), (1,2), (2,2)], 'tfidf__use_idf': (True, False), 'multiNB__alpha': np.logspace(-4,4,40)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs = -1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_words, np.array(y))\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Video Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('tfidf', TfidfTransformer(use_idf=False)),\n",
       "                ('lin_svc',\n",
       "                 LinearSVC(C=0.03359818286283781, class_weight='balanced'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take vidoes that have categories and train model based on them\n",
    "model = Pipeline(steps = [('vectorizer', CountVectorizer(ngram_range = (1,2))),\n",
    "                          ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                          ('lin_svc', LinearSVC(class_weight=\"balanced\", C = 0.03359818286283781))])\n",
    "\n",
    "model.fit(X_words_train, np.array(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.78\n",
      "Recall: 0.83\n",
      "F1: 0.78\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_test_pred)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, y_test_pred, average = 'macro')))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, y_test_pred, average = 'macro')))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, y_test_pred, average = 'macro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# then get all the videos (categorized and not) and predict them\n",
    "X_all = video_cls_data['words']\n",
    "y = video_cls_data['primary_category']\n",
    "\n",
    "prediction = model.predict(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_feature_space_ = video_cls_data_[['video_id', 'length']]\n",
    "video_feature_space_['category'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>length</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>624</td>\n",
       "      <td>466</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>620</td>\n",
       "      <td>137</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>787</td>\n",
       "      <td>285</td>\n",
       "      <td>Stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>771</td>\n",
       "      <td>305</td>\n",
       "      <td>Cognitive Behavioral Therapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2587</td>\n",
       "      <td>95</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>6464</td>\n",
       "      <td>144</td>\n",
       "      <td>Stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>6463</td>\n",
       "      <td>248</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>6449</td>\n",
       "      <td>182</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>6452</td>\n",
       "      <td>144</td>\n",
       "      <td>Stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>6450</td>\n",
       "      <td>254</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1023 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  length                      category\n",
       "0          624     466                    Depression\n",
       "1          620     137                       Anxiety\n",
       "2          787     285                        Stress\n",
       "3          771     305  Cognitive Behavioral Therapy\n",
       "4         2587      95                    Depression\n",
       "...        ...     ...                           ...\n",
       "1018      6464     144                        Stress\n",
       "1019      6463     248                    Depression\n",
       "1020      6449     182                    Depression\n",
       "1021      6452     144                        Stress\n",
       "1022      6450     254                    Depression\n",
       "\n",
       "[1023 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_feature_space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feature_space_.to_excel(\"video_id_to_cat.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feature_space = pd.get_dummies(video_feature_space_)\n",
    "video_feature_space = video_feature_space.set_index('video_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = video_feature_space['length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feature_space['length'] = video_feature_space['length'].apply(lambda x : x / mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feature_space.to_csv(\"video_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
